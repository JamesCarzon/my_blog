[
  {
    "path": "posts/08_16_21_third/",
    "title": "On answering the wrong question",
    "description": "Some examples of how statistical hypothesis testing sometimes answers \ndifferent questions than we would like to ask.",
    "author": [
      {
        "name": "James Carzon",
        "url": "https://jamescarzon.github.io/"
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nSometimes we find ourselves answering a different question than we’d been\r\nasked. We’ve all been there. One time I’d spent several minutes explaining the\r\noverlap in Pittsburgh bus route coverage to a freshman at CMU who had asked me\r\n“Where does the Carnegie Museum of Art sit among your favorite coffee\r\nshops?” So trust me, I’ve been there (and their cafe’s lattes are\r\nmiddle-of-the-road).\r\nAnother classic example of answering the wrong question is the interpretation\r\nof hypothesis tests in statistics. Say for the sake of discussion that I’m an\r\nordinary seismologist who has wandered into your home, and you ask me, “What’s\r\nthe probability that an earthquake happens in Yellowstone National Park this\r\nyear?” Bumbling and jabbering under my breath, I help myself to your dusty\r\nfloor and secure myself against the door so that no one may leave as I respond\r\nto your plainly stated question, answering: “Well, you see, within ninety-five percent error bars we can say with asymptotic certainty that that the\r\nlikelihood of a seismic event in Yellowstone in the next year is predicted to\r\nbe very unlikely with a confidence level of 0.05 and a \\(P\\)-value of 1.21.” You\r\nthink to yourself, are all seismologists like this? but you splurt out (rudely),\r\n“Hey, wait a minute! I didn’t ask you about the likelihood. I asked you\r\nabout the probability! And I certainly didn’t need you to conceal your sneaky\r\nsubstitution amongst uninteresting remarks on \\(P\\)-values and confidence\r\nlevels.” But alas, every scientist knows that a simply stated conclusion is a\r\nfalse one, and so every caveat known to be relevant to any statistical\r\nhypothesis ever must be rattled off upon its statement, like possible side\r\neffects to a new drug during a Super Bowl spot.\r\nIn this post, let’s ask some of the questions that we’re not allowed to ask in\r\nan introductory statistics class, the questions that scientists aren’t allowed\r\nto answer straightforwardly because their statistics profs say so. The\r\nhypothesis testing process that is so\r\nfundamental to the conduct of statistics in the real world - or so they would\r\nhave you believe1 -\r\nis actually such a rigid concept that the\r\nrules may seem unintuitive and the results, uninteresting. It is time now to\r\ndiscuss of these nagging issues.\r\nA preference for positive results\r\nWhen we first learn how to test hypotheses, we learn to test “simple”\r\nhypotheses. Let us start here.\r\nRecall the hypothesis testing process. A\r\nresearcher may ask whether it is likely the case that a treatment that they are\r\nadministering to a portion of their experiment participants is causing a\r\nsignificant difference in their health outcomes. The optimistic researcher\r\nwould like to see that the people receiving the treatment are turning out\r\nbetter than are the people receiving no treatment, and so they perform a\r\n“hypothesis test” with the “null hypothesis” that the treatment effect (call it\r\n\\(\\beta\\)) is not significant (i.e. \\(\\beta=0\\)). If we find that this hypothesis\r\nis very unlikely to be true given the data that we collect in the experiment,\r\nthen we will reject the hypothesis. If it is not very unlikely, then we will\r\nsay that we fail to reject the hypothesis and that it could be that the\r\ntreatment effect \\(\\beta\\) is not significant. We never “confirm” that \\(\\beta=0\\)\r\nbecause we cannot prove it to be true without perfect knowledge about how the\r\ntreatment affects people.\r\nTo test the hypothesis, we define a “test statistic”\r\nand compute the probability that some random variable takes on a value as\r\nextreme as the value of the test statistic. Which random variable we use\r\ndepends on our null hypothesis, but if this topic is unfamiliar, suffice it to\r\nsay that if the probability of obtaining as extreme a value as our test\r\nstatistic is really low,\r\nthen we may say that there’s enough evidence to conclude that the null\r\nhypothesis is probably false and that \\(\\beta\\neq 0\\). The treatment has an\r\neffect.\r\nThis is the typical approach to testing statistical hypotheses, but notice that\r\nthere are some seemingly arbitrary limitations to this highly specific process.\r\nFor example, if we are looking for a treatment effect, then we first pretend\r\nlike there is no effect and then test whether we can reject such a pretension.\r\nWhat if we wanted to test whether a treatment had no effect? Is that allowed?\r\nIt’s a little tricky. When we compute a test statistic, we compute it in such a\r\nway so that we can say what it’s probability distribution is only supposing\r\nthat the null hypothesis is true. For example, if our test statistic is the\r\naverage body temperature of a patient receiving the treatment, then under the\r\nassumption that the treatment causes no change in their condition, we can say\r\nthat their average body temparature should be distributed around an average\r\nvalue of about 98 degrees Fahrenheit. If we observe that the patients’\r\ntemperatures are far away from 98 on average, then it sure seems like the\r\ntreatment is having an effect!\r\nHowever, if instead we wanted to check whether the treatment did not have an\r\neffect, then we might try starting off with the null hypothesis that their\r\naverage temperature is not 98 degrees. If we can show that this\r\nhypothesis is unlikely to be true, then we can reject this claim and say that\r\nthere is not an effect. (If we used the hypothesis \\(\\beta=98\\), then the best we\r\ncould do with that test is “fail to reject” the claim that \\(\\beta=98\\) rather\r\nthan proving it.) So now we suppose that \\(\\beta\\neq 98\\). How do we test this?\r\nDo we pretend that \\(\\beta=100\\) and compare with our patients’ data? Why not\r\n\\(98.1\\)? Why not \\(98.001\\)? It is not clear how to compare our data with our\r\nhypothesis in this scenario. Thus the problem then arises that if you want to\r\nask a statistician for negative results, then you’re out of luck. Our\r\nhypothesis test process only knows how to handle questions which ask for\r\npositive results.\r\nUnphysical specificity\r\nIf, back when I was four years old, you (found me and) asked me if the number\r\n100 is a big number, then I probably would have told you, Yes, it is. If you\r\nasked ten-year-old me the same question, then I probably would have said no.\r\nBut it’s the same number! you tell me (a ten-year-old, now damaged by your\r\ncharmless consultation).\r\nIt would be in our interest to settle consistently, completely,\r\nonce-and-for-all what constitutes statistically “significant” evidence for or\r\nagainst a hypothesis. The difficulty that we face in doing this rigorously is\r\nthat there is no physical law which governs the cutoff point for good or bad\r\nevidence. At once there is the fact of the matter into which we inquire with\r\nour test - for example, there is a true and absolute body temperature which an\r\nexperiment participant should expect to experience as a result of their\r\nparticipation - and there is our limited view of the fact - limited by the\r\nnumber of people participating, by the precision of our thermometers, and so\r\non. There is no piece of technology capable of deciding from our limited view\r\nwhat the truth is, and there is no method of obtaining the truth by philosophy\r\nalone, either. The only propositional knowledge that we have that is not\r\nsubject to revision is the combination of impressions gained directly through\r\nthe senses as well as any statements made by the pope ex cathedra.\r\nFor this reason, when we test a hypothesis, we have to make our judgment on\r\nthat hypothesis with the knowledge that we are tolerating the possibility of\r\nmaking errors. In particular, there is a possibility of rejecting a hypothesis\r\nthat is actually true (which we call a Type I error) as well as of failing to\r\nreject a hypothesis that is actually false (which we call a Type II error).\r\nSuppose we establish for ourselves the principle that we should try to reject\r\nas many false hypotheses as we can in our time on Earth but not falsely reject\r\nmore than 5% of the true hypotheses. If we figure out the probability\r\ndistribution of a test statistic with which we can test our hypotheses, then\r\nlet’s just mark off the parts of the distribution which represent the most\r\nextreme 5% of values that the statistic can take. If the statistic for a test\r\nlands in one of those marked-off regions, then we will reject the hypothesis\r\nbeing tested. If it’s a bell curve-like distribution, then that looks something\r\nlike this:\r\n\r\n\r\n\r\n\r\n\r\n\r\nFigure 1: Some distribution with a hypothesis rejection region highlighted\r\n\r\n\r\n\r\nHowever, it somehow feels inappropriate to allow our treatment of evidence to\r\nbe this black and white. Should it be the case that test statistics which fall\r\njust barely outside of the rejection region lead to retained hypotheses while\r\ntest statistics which fall just barely inside lead to rejections? It doesn’t\r\nfeel like there’s a qualitative difference in small differences at the boundary\r\nwhen there isn’t a difference in small differences somewhere in the middle.\r\nThis uncomfortable fact about hypothesis testing merely helps to illustrate\r\none of the shortcomings of formal statements about statistical observations.\r\nIt’s not the case that we’ve proved anything interesting when we reject a\r\nhypothesis on some basis of statistical evidence. We are merely summarizing the\r\nhypothesis in some manner that is precise yet arbitrary. We have assigned some\r\nnumber to the hypothesis and we have described that number as being\r\n“rejectable” or “not rejectable,” but this particular concept of “rejectable”\r\nhere is so specific that it should not be mistaken for ordinary language terms\r\nlike “believable” or “interesting” or “possibly worth studying further.” It is\r\nuseful only insofar as it makes efficient and fairly consistent the process of\r\nactually deciding which hypotheses are believable, interesting, or possibly\r\nworth studying further.\r\nIndividuals don’t care about statistical confidence\r\nWhen one poses a hypothesis for statistical testing, there is an assumption\r\nthat their data points are all distributed similarly so that they can share\r\ninformation with each other and give some idea what the underlying distribution\r\nlooks like. This is the assumption of “independent and identically distributed\r\ndata” (or the i.i.d. assumption). In reality, this assumption is not obvious.\r\nFor example, in a medical study maybe a researcher would hope that the people who participate\r\nin the study share general traits so that no one has a wildly different\r\nreaction to a treatment and demonstrates an uncommon, unexpected side-effect.\r\nOtherwise, maybe they would hope that if any uncommon traits are represented,\r\nthen alternative traits are also represented, such as having both overweight and\r\nunderweight, old and young, healthy and ill participants, so that a difference\r\nin outcomes can be measured. These are all reasonable considerations for a\r\nresearcher who is trying to identify an average treatment effect, but they\r\ndon’t necessarily mean anything to an individual who faces the choice of\r\nwhether or not to receive the treatment.\r\nTo illustrate this point, consider the following scenario. A potassium\r\nsupplement is recommended to you by your doctor, and they assure you that it\r\nwill make you feel better with high confidence.2 How do they know? Well, the\r\nsupplement was administered to a large cohort of study participants and 95% of\r\nthose participants fell within an interval of “good” potassium levels in their\r\nblood, say in the whitened-out region of 1. Looking at\r\nthe curve, however, you note that at the same time about 95% of participants\r\nfell outside of the interval of “perfect” potassium levels, as depicted in\r\n2. Here is another 95% confidence set that your doctor\r\ncould have showed you which is as valid as the previous one yet emphasizes a\r\nless optimistic interpretation of the study outcomes. Being the shrewd\r\ninvestigator that you are, you attend to your fear of possibly being an outlier-\r\npatient and very likely not being among those most rewarded for receiving\r\ntreatment and forego the treatment.\r\n\r\n\r\n\r\nFigure 2: Some distribution with a weird hypothesis rejection region highlighted\r\n\r\n\r\n\r\nAre you wrong to make this judgment? as many people may do when they forego\r\nmedical interventions in fear of negative side effects. I answer that this is\r\nnot necessarily wrong, but if it seems at all odd that one would report on a\r\nnon-standard confidence interval of the form \\((-\\infty, L]\\cup[U,\\infty)\\) and\r\nlet their decision be determined by that interval, then it may be a fun\r\nexercise to convince yourself that a confidence interval of the form \\([L, U]\\)\r\nalways makes sense. Which cases do you want to capture with your interval? The\r\n“normal” ones or the “abnormal” ones?\r\n\r\nI’m really not accusing anyone in particular. There are\r\nnecessary constraints on how many times instructors of introductory statistics\r\nclasses can tell their students that this or that statistical method is\r\nactually wrong and not useful, and students sometimes just want to know if\r\ntheir answer is “correct” so that they can repeat their answers on the exam.↩︎\r\nSuppose your blood potassium\r\nlevels matter a lot for feeling well.↩︎\r\n",
    "preview": "posts/08_16_21_third/img/trains_passing_resize.png",
    "last_modified": "2022-06-21T15:54:49-04:00",
    "input_file": {}
  },
  {
    "path": "posts/07_05_21_second/",
    "title": "What does elo measure?",
    "description": "A definition of elo rating. An analysis of a chess game of mine. An inaugural \nentry in an amateur chess diary, if you will.",
    "author": [
      {
        "name": "James Carzon",
        "url": "https://jamescarzon.github.io/"
      }
    ],
    "date": "2021-08-16",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nIt is imperative that you establish your chess skill level as soon as possible. There is simply too much at stake to live your 21st century life in tactical darkness. If you seriously think that you can walk around inside this world with nothing but the minimal knowledge of how the pieces move and still hold your head high at night, then you’ve already lost. Mark my words, you will be the one left sitting on the curb outside Pizza Hut with your confidence shattered and your carpal tunnels throbbing while a confederacy of twelve-year-olds will be standing around you with their hands making knight-jump-shapes on their acne-ridden foreheads. There is no coming back from this disaster.\r\nAs a kid, I did not believe myself to be mathematically competent, and I assumed that good play of chess required some sort of talent for calculation. When I started playing chess in college, I marvelled at my opponents’ good fortune. I thought to myself as I lost time and again, I’d done nothing wrong! I traded pieces for pieces, I didn’t walk into check, and I kept my queen next to my king! How could I have lost?\r\nThe circumstances of the COVID-19 pandemic kindled a change in my attitude about the game. I found it to be an opportune activity to share remotely with my friends and a skill in which I was certainly capable of improving. The practice of learning to play better chess (as opposed to the practice itself of playing chess) is very satisfying now, because I no longer embarrass myself and have a renewed admiration for my ability to cling to mediocrity in a broad range of skills in which most people either find great success or total disdain.\r\nThis post is a reflection of my current state in the hobby of online chess. As an educational exercise and for curious readers, I want to document my thought process qualitatively a little. Additionally, in the interest of learning more about how my progress is measured, I investigate the “elo rating.” This investigation I share with you now.\r\nElo as proxy\r\nIt would be convenient to know your genuine world ranking in chess like how you know your height, but you can’t practically determine such a thing. The only way to decide how good people really are is to make everyone play against everyone else infinitely many times and then line up in such a way that no one is standing behind someone against whom they win more than half of the time. But such a tournament is impractical since we would not have anyone to wash our clothes or to clean after the horses on Mackinac Island, and so we will never know how good we really are.\r\nBut it is rather obvious that we may guess how good we are at the game, and to guess with improving certainty with more and more play, without ever actually measuring skill. To have a good guess, let’s just order everyone that you’ve played against according to your win percentage against them. You can stand in front of the people against whom you normally win, but you must stand behind those who beat you. How good is the person standing in front of you? And behind? Average their true rankings to approximate your own.\r\nOf course, this solution is no good since your opponents don’t know their true rankings either. Furthermore, it may be the case that two players against whom you consider yourself equally ranked do not consider themselves equally ranked, so this program of ordering everyone may be poorly defined.\r\nIn order to make up numbers to represent skill level, the elo rating system depends on the simple premise that a player’s true ability is not perfectly represented in any one game. Sometimes they play better than average and other times they play worse. Then, for example, if they are supposedly at rating \\(X\\) and they are winning frequently against players of ratings greater than \\(X\\), then this is a sign that their rating is underestimated and should be increased. Usually, a player’s performance is assumed to respect a curve that looks like the following graph.\r\n\r\n\r\n\r\nThis curve is of an “extreme value distribution,” also known as the Gumbel distribution. That is, it is a curve which shows the theoretical distribution of values of the maximum values of data samples which, in turn, are samples from some other distribution. A classic use case might be the following. Suppose a river runs through your property and you would like to know if the river is likely to rise enough to spill over your garden. You may know how high the river ran every day for the last ten years, but the only information that is relevant to you is the maximum height this year. Taking the maximum heights observed in each of the previous ten years as your data sample, you can compute their average and their standard deviation to find a Gumbel distribution which describes the probabilities of different ranges of river heights.\r\nIt is probably not true that anyone’s chess performance respects the Gumbel distribution. One could draw lots of other curves which would be just as believable descriptions of performance. However, the Gumbel distribution is a convenient choice for the purposes of a few calculations.\r\nIf I tell you that GM Autobus’ play is distributed like Gumbel random variable \\(A\\), with mean play at rating \\(2550\\), and FM Bindowcleaner’s play is distributed like \\(B\\), with mean play at \\(2150\\), then we should have the expectation that Autobus will beat Bindowcleaner fairly frequently. For the sake of making up some point of reference to explain how big this difference in rating is in terms of winning percentage, we decide as convention that a \\(+400\\) rating difference translates to probable winning odds of \\(10:1\\). So Autobus should be able to score 10 out of 11 points in an 11 game match against Bindowcleaner. In notation, write\r\n\\[\\frac{Pr(A\\text{ beats }B)}{Pr(B\\text{ beats }A)} = 10^{\\frac{R_A-R_B}{400}}.\\]\r\nOn the left is denoted the odds of a random variable pretending to be Autobus playing at a higher level than a random variable pretending to be Bindowcleaner. To say that “\\(A\\) beats \\(B\\)” is to say that \\(A\\) takes a number value higher than \\(B\\). On the right is an expression of those odds in terms of assumed rating; if the difference \\(R_A-R_B\\) is equal to \\(+400\\), then the righthand expression evaluates to \\(10\\). If \\(R_A-R_B=+800\\), then the righthand expression evaluates to \\(100\\). This makes sense. If Stockfish 14, the chess playing machine, has an assumed rating of \\(2950\\), then it should beat Autobus with odds 10 to 1. If it then faced off against Bindowcleaner, then it could do the following. It could say, “Hey GM Autobus! Why don’t you play 101 games for me against Bindowcleaner? If you win, then we’ll just say that I would have also won that game since I generally play better than you do. If you lose, however, then I will play the rematch against Bindowcleaner myself. Since I play better than you in 10 out of 11 games on average, I expect to be able to win 10 out of 11 of those 11 games that you are expected to lose to Bindowcleaner, thus preserving my legacy.” Ignoring the ethical difficulty of convincing a human to play for a computer, much less 101 games, the takeaway is that this equation describes the scenario which we have sought to describe.\r\nLet us algebraically extract some new information. Since it is vanishingly unlikely for \\(A\\) to equal \\(B\\) from a probability perspective, we will simply observe that either \\(A\\) beats \\(B\\) or \\(B\\) beats \\(A\\) in our scenario. Ignore draws. Then\r\n\\[Pr(A\\text{ beats }B)+Pr(B\\text{ beats }A)=100\\%=1,\\]\r\nso\r\n\\[Pr(B\\text{ beats }A)=1-Pr(A\\text{ beats }B).\\]\r\nCombining this with the first equation, we solve for \\(Pr(A\\text{ beats }B)\\) to obtain\r\n\\[Pr(A\\text{ beats }B) = \\frac{1}{1+10^{\\frac{R_B-R_A}{400}}}.\\]\r\nConveniently, this expression is related to our assumption that one’s chess performance follows a sort of curve like the Gumbel curve. The Gumbel curve is the graph of a function like\r\n\\[f(x) = \\frac{1}{\\beta}e^{-\\left(\\frac{x-\\mu}{\\beta}+e^{\\frac{x-\\mu}{\\beta}}\\right)},\\]\r\nwhere the number \\(\\mu\\) determines where the curve is centered and the number \\(\\beta\\) determines the horizontal scale. Let us take it as a fact now that if two random variables \\(A\\) and \\(B\\) follow Gumbel distributions with the same scale number \\(\\beta\\), then the random variable \\(B-A\\) is a random variable with a logistic distribution, meaning that it has a distribution defined by the function\r\n\\[g(x) = \\frac{e^{-\\frac{x-\\mu*}{\\beta}}}{\\beta\\left(1+e^{-\\frac{x-\\mu*}{\\beta}}\\right)^2},\\]\r\nwhere \\(\\mu*\\) is the center of the curve showing the distribution of \\(B-A\\). If we perform a clever calculus calculation, then we can obtain\r\n\\[Pr(B-A\\le x) = \\int_{-\\infty}^{x}f(y)dy = \\frac{1}{1+e^{-\\frac{x-\\mu*}{\\beta}}}.\\]\r\nThis function, this particular probability, is a function which looks similar to our expression for \\(Pr(A\\text{ beats }B)\\).\r\nIn particular, if \\(\\mu_*=R_A-R_B\\) and \\(\\beta=\\frac{400}{\\log 10}\\), then\r\n\\[P(A\\text{ beats }B) = P(B-A\\leq 0) = \\frac{1}{1+e^{\\left[-\\frac{R_B-R_A}{\\frac{400}{\\log 10}}\\right]}} = \\frac{1}{1+10^{-\\frac{R_B-R_A}{400}}}.\\]\r\nThis calculation is repeated from Arthur Berg’s explanation in Chance (Berg 2020). So it seems that this probability model for chess play is somehow justified by the convenience of this calculation. If we pretend that one’s performance follows this Gumbel curve, then we can say that the odds seen in games are structured in the way described concretely above: a \\(+400\\) rating difference translates to expected \\(10:1\\) odds of winning. To conclude, Elo doesn’t measure chess ability or ranking, but it is a somewhat sophisticated guess as to one’s relative winning odds against another player.\r\nDon’t trot out the queen\r\nUnderstanding the technical definition of chess rating has not been practically helpful for playing better chess in my experience, but I have learned a few things about how to adjust my play to opponents of different ratings. For example, I recognized early on in my career as an amateur that low rated players are forever begging to trick someone into falling for the “Scholar’s Mate.” Something like this is what they have in mind:\r\n\r\n\r\n\r\nIt’s a quick and annoying loss for the poor soul who doesn’t see it coming. However, in order to achieve this trick, you have to breach a generally true rule: you have to develop our queen piece early. This is typically a bad idea because it is easy for your opponent to simultaneously get their pieces into the center of the board while also attacking your queen. As you run the queen away over and over again, your other pieces don’t move and you end up falling behind. For this reason, it’s better to move your other pieces out before the queen.\r\nI think that it is at or around the 1000 elo rating in online chess play that everyone is smart enough not to try using the Scholar’s Mate as an old, reliable tool. Similar patterns do emerge sometimes. As an example that I may pleasantly share, I submit for your consideration the following game that I played against an opponent by the handle “PunisherFevzican9.”\r\n\r\n\r\nThe initial move 1. … c5 is the Sicilian Defense. Up to 4. Bc4, this game is a common position, but with 4. … f6 the database on Lichess.org is exhausted. This pawn push is considered a mistake because it allows me to break into the center, but it was not obvious to either of us in that moment. I just happened to play the best move next.\r\nWe trade pieces in the center on d4 until 7. Qxd4. Now my queen is developed and easily be attacked, and so comes 7. … d5. Unfortunately for my opponent, this gives me the option to attack the vacated f7 square with a bishop-queen battery, almost like how the Scholar’s Mate looks, except now my queen can’t be dislodged. All the pawns are too far advanced. I play 8. Qd5, eying f7. Then 8. … Qe7 defends that square, so I bring my knight to b5, now getting ready to fork the king and rook or maybe just help in the attack. The black queen now runs around trying to find something dangerous to do but can’t figure anything out. With 13. Qxe7+, I trade queens, thinking that I have a strong enough attack without the queens on the board to checkmate soon.\r\nAt this point I spend some time looking for a possible checkmate pattern. I saw that the king had two squares to move to: c7 and e8. If I could hold the king in place, then that would make it hard for black to develop the bishop on c8 that’s stuck behind two pawns and, thus, the rooks stuck inside their own caves.\r\nWith 14. Rd1, I enable one more piece to participate. Trading a knight and bishop, black makes some room, but this is a distraction from my own plan. As black plays 16. … Ne7, the king is once again limited to the squares c7 and e8, and my bishops are perfectly set up to close these off. 17. Bf7 covers e8. Black simply looks at the bishop with 17. … Rf8. Then 18. Bb6 is mate.\r\nI did not play an excellent game. Many moments of this attack were refutable. For example, if after 17. Bf7 the black king moved to c7, I would have been a bit stuck. However, this was a fast game, played with 5 minutes plus 3 second increments on each move, so my opponent didn’t necessarily have a chance to calculate my plan at each turn. The lesson that I learned from this game is that an aggressive attack can work, even against players who are familiar with some tricks, if the attacking plan is specific and coordinated with the right mistakes made by your opponent. So don’t just trot out the queen! Let your attacks be sound and games be exciting.\r\n\r\n\r\n\r\nBerg, Arthur. 2020. “Statistical Analysis of the Elo Rating System in Chess.” Chance Magazine, September. https://chance.amstat.org/2020/09/chess/.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/07_05_21_second/img/elo_rating_resize.png",
    "last_modified": "2021-08-16T15:48:16-04:00",
    "input_file": {}
  },
  {
    "path": "posts/06_21_21_first/",
    "title": "Self-fulfilling prophecies and Big Data",
    "description": "A look at inequality as it's driven by algorithmic thinking. Some advertising \ndata investigation. A discussion of the book \"Weapons of Math Destruction\" \nby Cathy O'Neil.",
    "author": [
      {
        "name": "James Carzon",
        "url": "https://jamescarzon.github.io/"
      }
    ],
    "date": "2021-06-21",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nA distinction can be made between the rigorous nature of mathematics and the fuzziness of its applications. Mathematical claims are proved logically and totally. Everything else, one might believe, is only proved to one’s satisfaction or not, but never irrevocably. In the process of translating between Science and Math, something perfect is lost.\r\nThis distinction is a philosophical preface to a long debate on addressing “racist algorithms,” “sexist science,” and so forth. There is a sense in which Big Data are understood to excite inequalities such as wage gaps between racial and gender groups or gaps in job application successes and graduation rates for socioeconomic groups. The gap between science and math constitutes the most immediate criticism of such claims to systemic or social inequality (whatever that means generally) caused (in entirety or in part) by “Big Data.” You know–numbers and stuff. Math. Perfect truth, and the facts that we find when we study the numbers and stuff that are generated and documented by modern technology at an incredible volume.\r\nIt certainly seems to me that these data, like any other data, have no idea what they are describing and that they themselves have no intent to cause anything. They simply arise in the world naturally. It is not the case that “the number of people in academia who consistently vote for Republicans” might be one of perhaps several existing entities conspiring to steepen political bias in higher education. It’s just (possibly) a fact which we (might) observe. So to claim that Big Data are a reason for some sort of inequality of this sort is, obviously, a misunderstanding of what they are.\r\nHowever, there are fairly clear and uncontroversial examples of negative feedback loops which are uniquely enabled by our uses of Big Data which can serve as a clue to the nature of more impactful claims. That is, there are ways in which data and their trained algorithms help to widen measurable inequalities. I have written this post in an attempt to clarify this point of confusion from my personal understanding of the phenomenon of data-driven feedback loops. The idea came to me when I read Cathy O’Neil’s book Weapons of Math Destruction, whose subject matter is the exposure of examples of precisely these sorts of cases. I consider this subject to be one which is too easily dismissed on the basis that it participates in this Math v. Science fallacy, which I hold it does not.\r\nStereotypes arising from data\r\nI am a cautious data scientist, say. I would like to be able to chase a claim from some data set to its source and totally and completely understand the story of that data set’s generation. That would be the ideal way to “do statistics about the world.” This is difficult to do.\r\nFor example, maybe I want to know how my online advertisements are determined. I observe the advertisements which are fed to me and find myself surprised on occasion by which ones I see. Is my phone really spying on me when I’m eating the last of my baby carrots only to inform Google to bump a Domino’s Pizza spot on my Jordan Peterson lecture on YouTube? As a casual experiment, I jumped onto The Grad Cafe, a forum for graduate students from across all of academia, and surveyed the ads I received after a few minutes of browsing: AllState, Hewlett-Packard, Go Daddy, Xfinity and Spectrum, T-Mobile, UWMCareers.com, and the University of Connecticut Pharmacy School. Why do you think I am getting these ads in particular?\r\nOn the one hand, my personal browsing history would suggest that I have spent time recently thinking about insurance and the internet and building websites, et cetera. These are topics that I’ve undoubtedly binge-Binged before. It is plausible that I am shown ads which, among those paid for by companies across the internet, were determined to be most interesting to me by some algorithm.\r\nOn the other hand, I am a user on The Grad Cafe after all and, as such, fit into the demographic of people interested in graduate school. Like many of my peers on the site, I’m in my early ’20s and I have a college degree. I have the time and resources to browse the internet freely and at length. People like me tend to be interested in things like academic careers, pharmacy schools, and cloud computing. It is plausible that I am shown ads which, among those paid for by companies across the internet, were determined to be most interesting to us users of The Grad Cafe.\r\nSince both of these explanations are plausible ones for some or all of the ads I see, I can’t simply x-ray through the results and understand how these choices are being made. I could be judged according to my personal history or I could be judged according to the history of my cohort. In a complex way, I may be judged according to a combination of these.\r\n\r\n\r\n\r\nYou should know that you are welcome to investigate the character of Google’s advertising profile of you. An ad that is provided by Ads by Google should show a triangle button in the top right corner which, when clicked, sends you to an ads settings page. From there you can find a collection of interests for which you’ve been historically pegged.\r\n\r\n\r\n\r\nToday I found out that I am a grains & pasta guy. I did not know this about myself, and had I been adequately tuned in to my body’s cravings I would have hesitated before committing to a keto cleanse that threw me out of lawn maintenance commission for a week and a half of yoga in bed with the lights off. Now I’ll think twice before cutting bread out of my diet, then three times, then four times before caving to a bowl of Frosted Mini Wheats that I can’t finish but do anyways.\r\n\r\n\r\n\r\nThere are several reasons why I have never lost sleep over the threat of an unplanned pregnancy upsetting my assumed way of life, but none of them are that I fail to sense the coming urge of regular bodily functions which, if nightly active, would surely cause me to lose some sleep. Have I forgotten some vast library of videos of clumsy toddlers that my grandma asked me to back up for her in case the family inheritance is pinched by a sleeper cell of the Irish mafia? Maybe that’s an anonymous Reddit account that I’ve forgotten about.\r\n\r\n\r\n\r\nIf you know me, then you know that I need my daily fix of humidifier / dehumidifier mod and app developer news sent to my inbox to start the morning off on a good note. I’m not ashamed to admit that all of my Twitch subscriptions are to water filter and purifier unboxing channels.\r\nThe stereotype that Google has of me is not totally correct, but I saw a bunch of interests which were accurate. This discovery was insightful for me because it inforces a general rule about stereotypes: the farther away from the individual that the data comes, the less accurate its conclusions can dependably be for the individual.\r\nThe negative feedback loop\r\nOver time, Google has inferred these interests of mine from my activity indirectly. As long as the algorithm satisfies me, I reinforce the stereotype. In this example of online advertising, my stereotype is largely benign, but hopefully it is clear that the concept at work can be translated to different contexts. Stereotypes of all kinds are like algorithms. They efficiently approximate the data that we collect, but with that efficiency comes a compromise in accuracy. When we make efficient approximations of someone’s behavior, for example, based on the groups to which they belong, we lose information about their individual peculiarities. This claim, I think, is clear enough.\r\nWhat is the value of a stereotype? If this is a purely sociological question, then it’s difficult to answer it as a statistician. The threats of “Big Data” are well noted, though. Cathy O’Neil’s book Weapons of Math Destruction furnishes a collection of more serious examples of algorithms which are the products of an attitude that squeezing meaning out of Big Data is the way of the future for sociological problems.\r\n\r\n\r\n\r\nFor example, one might look at university rankings to decide to which colleges they should apply, but there is nothing “true” about the University of Virginia being a better place to study than the University of Florida. Maybe if you asked a billion people which one is better, then you can bet that they will favor Virginia in a mass, but that’s not corresponding to any facts of the universe! The US News staff doesn’t count the goodness particles floating in the Gainesville sky and weigh its ranking on that metric. Yet it’s easier for us to steamroll ahead with decisions informed by numbers than to perform a cost-benefit analysis with even less complete data like impressions that we glean from a website and maybe a conversation or two; it’s also easier for university administrators to invest in those features of their university which are relevant to their ranking than to make personal judgment calls on what most needs fixing. Of course, I am now indulging in an oversimplification of both of these processes.\r\nThe assumption underlying these applications of Big Data is that people behave in accordance with categories. Political groups cohere in their decisions, this assumption further suggests, as do ethnic and religious and socioeconomic ones, and so they will be treated according to their group. The problem with this assumption is two-fold. Firstly, it can sharpen ideological boundaries by refining echo chambers and quelling cross-pollination. For example, if politically radical individuals are only recommended politically radical YouTube videos, and if those same videos are rarely recommended to politically opposed individuals, then the consumers of that content may falsely believe their opinions to be widely accepted and not reasonably opposed.\r\nSecondly, it allows for centralized authority. For example, if schoolteachers’ efficacy can be judged by exam scores, then one person with a computer can decide who in a district should be fired by crunching the exam data set and ignoring unmeasurable facts of the exam and the settings in which it was administered. To let all decisions like these be made algorithmically is to decide from the outset that the truth of a judgment is less important than the specificity of it. When authority is centralized, these facts may be lost. So, enabling (and ennobling) algorithms with negative feedback loops of this kind for the purpose of increasing efficiency invites division and ignores local and individual economic peculiarities.\r\nThese two issues seem to invite plenty of extrapolation into political realms which I will not try to do here because I have not even argued for an ethical conclusion to the phenomenon of data-driven feedback loops. I did use the adjective “negative,” which is suggestive, but it’s not an argument. The problem of algorithmic thinking is not “my problem with algorithms.” I am only clarifying an idea which sees itself in these relevant debates.\r\n\r\n\r\n\r\n",
    "preview": "posts/06_21_21_first/img/book_review.png",
    "last_modified": "2021-08-16T13:59:41-04:00",
    "input_file": {}
  }
]
